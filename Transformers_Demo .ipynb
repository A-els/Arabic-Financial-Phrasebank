{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a154c11589e4f7e8732c5fb3edbba2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7568ab6b47b140dc9825cb80f00b0592",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8e63e3642b9445ff9cfb84a7dca8dbcd",
              "IPY_MODEL_c65a96942006427cacda12e579b37877"
            ]
          }
        },
        "7568ab6b47b140dc9825cb80f00b0592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e63e3642b9445ff9cfb84a7dca8dbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_772ec3c740dd42d787fd1b9263feadac",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c76c90a337934e1f9756f74e0cc66e32"
          }
        },
        "c65a96942006427cacda12e579b37877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_47c215d562524a3cbc766be7fd7216c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 9.06kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e91b15646ea46ba975d97303014d65a"
          }
        },
        "772ec3c740dd42d787fd1b9263feadac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c76c90a337934e1f9756f74e0cc66e32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "47c215d562524a3cbc766be7fd7216c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e91b15646ea46ba975d97303014d65a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "006a5c7d99f04700b984f5526450f3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e1a51a8ae6a4b7584dd05e8e0e1d4d2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c2852cbb58a4b1e84e548d427e983f0",
              "IPY_MODEL_8457885a8086465795f9afcdda997694"
            ]
          }
        },
        "1e1a51a8ae6a4b7584dd05e8e0e1d4d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c2852cbb58a4b1e84e548d427e983f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_553701da1aae495aa30f6f209db07658",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd68e7fe23bf4e909cafab5ea4342bfb"
          }
        },
        "8457885a8086465795f9afcdda997694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f92d2c6987d64410b4ad72579526435c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:03&lt;00:00, 62.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07346fcfb0c241f99cf81169df35dffd"
          }
        },
        "553701da1aae495aa30f6f209db07658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd68e7fe23bf4e909cafab5ea4342bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f92d2c6987d64410b4ad72579526435c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07346fcfb0c241f99cf81169df35dffd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f91154580c2c43e8a1f2bc8f09037d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1926e4c919004bd2ba186869f5eed83f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_759f51dd5dd443e1842f88c4bc05c0c9",
              "IPY_MODEL_6fe8a0d68fce4100bc8d8e7ba8d45046"
            ]
          }
        },
        "1926e4c919004bd2ba186869f5eed83f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "759f51dd5dd443e1842f88c4bc05c0c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d5afc3e758b944b9b2c06ef988629a73",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d11097f1ff934d96a924d869f085f149"
          }
        },
        "6fe8a0d68fce4100bc8d8e7ba8d45046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b681cc51caa14b3ebe85e6c8539d600b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:03&lt;00:00, 155kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b043c7550a104168a26b3b7276fb031d"
          }
        },
        "d5afc3e758b944b9b2c06ef988629a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d11097f1ff934d96a924d869f085f149": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b681cc51caa14b3ebe85e6c8539d600b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b043c7550a104168a26b3b7276fb031d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dddb62485b44a20977ad93afd114757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b45427a19cef4282ad0279e2cc0a224a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c353c124d9841178a1c3837f080f533",
              "IPY_MODEL_7065882e2b754d419f8a53b20c3e6b0f"
            ]
          }
        },
        "b45427a19cef4282ad0279e2cc0a224a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c353c124d9841178a1c3837f080f533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c309fcf97cc040169e6cea9e97bdfe41",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45cc1b52aa6943698eb96b7c3cb34195"
          }
        },
        "7065882e2b754d419f8a53b20c3e6b0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_60b66500112149fabc13f2c787eac11d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:07&lt;00:00, 57.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f2cb52b8c914fe6b58d138324b36749"
          }
        },
        "c309fcf97cc040169e6cea9e97bdfe41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45cc1b52aa6943698eb96b7c3cb34195": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "60b66500112149fabc13f2c787eac11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f2cb52b8c914fe6b58d138324b36749": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-WdbF4MFHxh"
      },
      "source": [
        "# Dependencies\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfxKE7uE8Iep",
        "outputId": "39cbf643-5a3f-4be0-c593-9628248763ab"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "    !nvidia-smi\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Mon Mar  1 12:25:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf1dzVo28Rjo",
        "outputId": "77fff4fe-e357-4a08-c752-8ae9c87b189f"
      },
      "source": [
        "!pip install transformers==4.2.1\r\n",
        "!pip install optuna==2.3.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/40/866cbfac4601e0f74c7303d533a9c5d4a53858bd402e08e3e294dd271f25/transformers-4.2.1-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (3.7.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (2019.12.20)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/36/59e4a62254c5fcb43894c6b0e9403ec6f4238cc2422a003ed2e6279a1784/tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 49.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.2.1) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.2.1) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=b2ace620d7a7b2ad7912c7abcd81026034382e2837809a4e01590c7960371e48\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.1\n",
            "Collecting optuna==2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/10/06b58f4120f26b603d905a594650440ea1fd74476b8b360dbf01e111469b/optuna-2.3.0.tar.gz (258kB)\n",
            "\u001b[K     |████████████████████████████████| 266kB 7.9MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (4.41.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.3.23)\n",
            "Collecting cliff\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/d6/7d9acb68a77acd140be7fececb7f2701b2a29d2da9c54184cb8f93509590/cliff-3.7.0-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.8MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/01/1f/43b01223a0366171f474320c6e966c39a11587287f098a5f09809b45e05f/cmaes-0.8.2-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.19.5)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.4.1)\n",
            "Collecting alembic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/07/799a76aca0acd406e3259cc6c558ca1cdadf88250953b6c8105b421a9e33/alembic-1.5.5.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 27.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (1.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna==2.3.0) (20.9)\n",
            "Collecting colorlog\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/39/0230290df0519d528d8d0ffdfd900150ed24e0076d13b1f19e279444aab1/colorlog-4.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (2.4.7)\n",
            "Collecting cmd2>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/8b/15061b32332bb35ea2a2f6263d0f616779d576e82739ec8e7fcf3c94abf5/cmd2-1.5.0-py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 51.1MB/s \n",
            "\u001b[?25hCollecting stevedore>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/49/b602307aeac3df3384ff1fcd05da9c0376c622a6c48bb5325f28ab165b57/stevedore-3.3.0-py3-none-any.whl (49kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (2.0.0)\n",
            "Collecting pbr!=2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/48/69046506f6ac61c1eaa9a0d42d22d54673b69e176d30ca98e3f61513e980/pbr-5.5.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna==2.3.0) (3.13)\n",
            "Collecting Mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/db/2d2d88b924aa4674a080aae83b59ea19d593250bfe5ed789947c21736785/Mako-1.1.4.tar.gz (479kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 49.0MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna==2.3.0) (2.8.1)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/2c/4c64579f847bd5d539803c8b909e54ba087a79d01bb3aba433a95879a6c5/pyperclip-1.8.2.tar.gz\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (0.2.5)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (20.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna==2.3.0) (3.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from PrettyTable>=0.7.2->cliff->optuna==2.3.0) (53.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna==2.3.0) (1.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna==2.3.0) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna==2.3.0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2>=1.0.0->cliff->optuna==2.3.0) (3.7.4.3)\n",
            "Building wheels for collected packages: optuna\n",
            "  Building wheel for optuna (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-2.3.0-cp37-none-any.whl size=359761 sha256=0a45409109cdd1f043ce0c8868295ff79fe2c1d47fb52d6dc9d85a2754654d54\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/91/19/64b0ec6b964f89c0695a9dc6db6f851d0b54c5381a5c9cadfb\n",
            "Successfully built optuna\n",
            "Building wheels for collected packages: alembic, Mako, pyperclip\n",
            "  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.5.5-py2.py3-none-any.whl size=156597 sha256=a583ef815aacb7ea653927b8a013802a5efde375481c55c67c6ff12a07046125\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/3f/61/7de6e3cef766d2680a5d81b1a388286e640f6a681eb589d643\n",
            "  Building wheel for Mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=75675 sha256=385fd6b3e69b9629a44c9ce87fdad90cabfc4b3fd635d05ef0f061f3884f04f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/10/d3/aeb26e20d19045e2a68e5d3cbb57432e11b5d9c92c99f98d47\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-cp37-none-any.whl size=11107 sha256=d42e3b3c10a828b40639688b541ec2253cc7e31023ea2b71ca86df9d91d47e6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/af/b8/3407109267803f4015e1ee2ff23be0c8c19ce4008665931ee1\n",
            "Successfully built alembic Mako pyperclip\n",
            "Installing collected packages: pyperclip, colorama, cmd2, pbr, stevedore, cliff, cmaes, Mako, python-editor, alembic, colorlog, optuna\n",
            "Successfully installed Mako-1.1.4 alembic-1.5.5 cliff-3.7.0 cmaes-0.8.2 cmd2-1.5.0 colorama-0.4.4 colorlog-4.7.2 optuna-2.3.0 pbr-5.5.1 pyperclip-1.8.2 python-editor-1.0.4 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoJKPIr08S07"
      },
      "source": [
        "!mkdir data\r\n",
        "!mkdir train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjfyp2NCFP_w"
      },
      "source": [
        "# Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r9ytUfL87xG"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "all_datasets= []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcZPXfmF89tU"
      },
      "source": [
        "class Dataset:\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        name,\r\n",
        "        train,\r\n",
        "        test,\r\n",
        "        label_list,\r\n",
        "    ):\r\n",
        "        self.name = name\r\n",
        "        self.train = train\r\n",
        "        self.test = test\r\n",
        "        self.label_list = label_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuaUNp-99ALr"
      },
      "source": [
        "DATA_COLUMN = \"text\"\r\n",
        "LABEL_COLUMN = \"label\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4fabDlb9DKQ",
        "outputId": "fe7bfcb4-76ff-4fde-9dd2-83154f18923f"
      },
      "source": [
        "df = pd.read_csv('Sentences_AllAgree.txt', sep=\"@\", encoding ='ISO-8859-1' , header=None)\r\n",
        "df.columns = [DATA_COLUMN, LABEL_COLUMN]\r\n",
        "print(df)\r\n",
        "print(df[LABEL_COLUMN].value_counts())\r\n",
        "\r\n",
        "train_split, test_split = train_test_split(df, test_size=0.2, random_state=42)\r\n",
        "label_list = list(df[LABEL_COLUMN].unique())\r\n",
        "print(label_list)\r\n",
        "\r\n",
        "data_All_Agree = Dataset(\r\n",
        "    \"All_Agree\", train_split, test_split, label_list\r\n",
        ")\r\n",
        "\r\n",
        "all_datasets.append(data_All_Agree)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   text     label\n",
            "0     According to Gran , the company has no plans t...   neutral\n",
            "1     For the last quarter of 2010 , Componenta 's n...  positive\n",
            "2     In the third quarter of 2010 , net sales incre...  positive\n",
            "3     Operating profit rose to EUR 13.1 mn from EUR ...  positive\n",
            "4     Operating profit totalled EUR 21.1 mn , up fro...  positive\n",
            "...                                                 ...       ...\n",
            "2259  Operating result for the 12-month period decre...  negative\n",
            "2260  HELSINKI Thomson Financial - Shares in Cargote...  negative\n",
            "2261  LONDON MarketWatch -- Share prices ended lower...  negative\n",
            "2262  Operating profit fell to EUR 35.4 mn from EUR ...  negative\n",
            "2263  Sales in Finland decreased by 10.5 % in Januar...  negative\n",
            "\n",
            "[2264 rows x 2 columns]\n",
            "neutral     1391\n",
            "positive     570\n",
            "negative     303\n",
            "Name: label, dtype: int64\n",
            "['neutral', 'positive', 'negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBkqk_HG98dX",
        "outputId": "714e4f22-9efb-42f6-b7e9-4c1da1facbca"
      },
      "source": [
        "for x in all_datasets:\r\n",
        "  print(x.name) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All_Agree\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmQU7tjFbva"
      },
      "source": [
        "# BERT compliant Dataset and model_init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct7gjWAMDRq8"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\r\n",
        "\r\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, BertTokenizer\r\n",
        "from transformers.data.processors import SingleSentenceClassificationProcessor\r\n",
        "from transformers import Trainer , TrainingArguments\r\n",
        "from transformers.trainer_utils import EvaluationStrategy\r\n",
        "from transformers.data.processors.utils import InputFeatures\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from sklearn.utils import resample\r\n",
        "import logging\r\n",
        "import torch\r\n",
        "import optuna \r\n",
        "\r\n",
        "logging.basicConfig(level=logging.WARNING)\r\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhG45G6ADmxA"
      },
      "source": [
        "dataset_name = 'All_Agree'\r\n",
        "model_name = 'bert-base-uncased'\r\n",
        "task_name = 'classification'\r\n",
        "max_len = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFndYSywEBSB",
        "outputId": "d507fa04-dbf3-4051-9cd3-db15b5ca8ebd"
      },
      "source": [
        "for d in all_datasets:\r\n",
        "  if d.name==dataset_name:\r\n",
        "    selected_dataset = d\r\n",
        "    print(d.name)\r\n",
        "    print('Dataset found')\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All_Agree\n",
            "Dataset found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CyfGyvSEJad"
      },
      "source": [
        "class BERTDataset(Dataset):\r\n",
        "    def __init__(self, text, target, model_name, max_len, label_map):\r\n",
        "      super(BERTDataset).__init__()\r\n",
        "      self.text = text\r\n",
        "      self.target = target\r\n",
        "      self.tokenizer_name = model_name\r\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "      self.max_len = max_len\r\n",
        "      self.label_map = label_map\r\n",
        "      \r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "      return len(self.text)\r\n",
        "\r\n",
        "    def __getitem__(self,item):\r\n",
        "      text = str(self.text[item])\r\n",
        "      text = \" \".join(text.split())\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "      input_ids = self.tokenizer.encode(\r\n",
        "          text,\r\n",
        "          add_special_tokens=True,\r\n",
        "          max_length=self.max_len,\r\n",
        "          truncation='longest_first'\r\n",
        "      )     \r\n",
        "    \r\n",
        "      attention_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "      # Zero-pad up to the sequence length.\r\n",
        "      padding_length = self.max_len - len(input_ids)\r\n",
        "      input_ids = input_ids + ([self.tokenizer.pad_token_id] * padding_length)\r\n",
        "      attention_mask = attention_mask + ([0] * padding_length)    \r\n",
        "      \r\n",
        "      return InputFeatures(input_ids=input_ids, attention_mask=attention_mask, label=self.label_map[self.target[item]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184,
          "referenced_widgets": [
            "6a154c11589e4f7e8732c5fb3edbba2c",
            "7568ab6b47b140dc9825cb80f00b0592",
            "8e63e3642b9445ff9cfb84a7dca8dbcd",
            "c65a96942006427cacda12e579b37877",
            "772ec3c740dd42d787fd1b9263feadac",
            "c76c90a337934e1f9756f74e0cc66e32",
            "47c215d562524a3cbc766be7fd7216c7",
            "6e91b15646ea46ba975d97303014d65a",
            "006a5c7d99f04700b984f5526450f3ae",
            "1e1a51a8ae6a4b7584dd05e8e0e1d4d2",
            "4c2852cbb58a4b1e84e548d427e983f0",
            "8457885a8086465795f9afcdda997694",
            "553701da1aae495aa30f6f209db07658",
            "fd68e7fe23bf4e909cafab5ea4342bfb",
            "f92d2c6987d64410b4ad72579526435c",
            "07346fcfb0c241f99cf81169df35dffd",
            "f91154580c2c43e8a1f2bc8f09037d7d",
            "1926e4c919004bd2ba186869f5eed83f",
            "759f51dd5dd443e1842f88c4bc05c0c9",
            "6fe8a0d68fce4100bc8d8e7ba8d45046",
            "d5afc3e758b944b9b2c06ef988629a73",
            "d11097f1ff934d96a924d869f085f149",
            "b681cc51caa14b3ebe85e6c8539d600b",
            "b043c7550a104168a26b3b7276fb031d"
          ]
        },
        "id": "borF8Ay2ESWZ",
        "outputId": "b0709173-ddd4-47c0-d8be-62764d940755"
      },
      "source": [
        "label_map = { v:index for index, v in enumerate(selected_dataset.label_list) }\r\n",
        "print(label_map)\r\n",
        "train_dataset = BERTDataset(selected_dataset.train[DATA_COLUMN].to_list(),selected_dataset.train[LABEL_COLUMN].to_list(),model_name,max_len,label_map)\r\n",
        "test_dataset = BERTDataset(selected_dataset.test[DATA_COLUMN].to_list(),selected_dataset.test[LABEL_COLUMN].to_list(),model_name,max_len,label_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'neutral': 0, 'positive': 1, 'negative': 2}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a154c11589e4f7e8732c5fb3edbba2c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "006a5c7d99f04700b984f5526450f3ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f91154580c2c43e8a1f2bc8f09037d7d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSw_JYacEbyz"
      },
      "source": [
        "def model_init():\r\n",
        "    return AutoModelForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=len(label_map))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gLTc28rFnEk"
      },
      "source": [
        "The compute_metrics function can be used to compute custom metrics during training/evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5nx2oCPEdOy"
      },
      "source": [
        "def compute_metrics(p): #p should be of type EvalPrediction\r\n",
        "  preds = np.argmax(p.predictions, axis=1)\r\n",
        "  assert len(preds) == len(p.label_ids)\r\n",
        "  #print(classification_report(p.label_ids,preds))\r\n",
        "  #print(confusion_matrix(p.label_ids,preds))\r\n",
        "\r\n",
        "  macro_f1_pos_neg = f1_score(p.label_ids,preds,average='macro',labels=[1,2])\r\n",
        "  macro_f1 = f1_score(p.label_ids,preds,average='macro')\r\n",
        "  macro_precision = precision_score(p.label_ids,preds,average='macro')\r\n",
        "  macro_recall = recall_score(p.label_ids,preds,average='macro')\r\n",
        "  acc = accuracy_score(p.label_ids,preds)\r\n",
        "  return {\r\n",
        "      'macro_f1' : macro_f1,\r\n",
        "      'macro_f1_pos_neg' : macro_f1_pos_neg,  \r\n",
        "      'macro_precision': macro_precision,\r\n",
        "      'macro_recall': macro_recall,\r\n",
        "      'accuracy': acc\r\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdt0KjbwF72r"
      },
      "source": [
        "# Hyper Parameter Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdl68KOiEiuD"
      },
      "source": [
        "training_args = TrainingArguments(\"./train\")\r\n",
        "training_args.evaluate_during_training = True\r\n",
        "training_args.adam_epsilon = 1e-8\r\n",
        "training_args.lr_scheduler_type = 'cosine'\r\n",
        "training_args.fp16 = True\r\n",
        "training_args.per_device_train_batch_size = 16\r\n",
        "training_args.per_device_eval_batch_size = 16\r\n",
        "training_args.gradient_accumulation_steps = 2\r\n",
        "training_args.num_train_epochs= 8\r\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\r\n",
        "# training_args.logging_steps = 200\r\n",
        "training_args.save_steps = 100000\r\n",
        "# training_args.save_steps = \r\n",
        "#training_args.eval_steps = \r\n",
        "training_args.disable_tqdm = True\r\n",
        "# print(\"Logging Step:\", training_args.logging_steps)\r\n",
        "# print(\"Eval Step:\",training_args.eval_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hwd4wtcEkZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488eb91c-d11f-4081-beac-f94771bbb2cc"
      },
      "source": [
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\r\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\r\n",
        "print(steps_per_epoch)\r\n",
        "print(total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56\n",
            "448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MHeMdXxEmAK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "1dddb62485b44a20977ad93afd114757",
            "b45427a19cef4282ad0279e2cc0a224a",
            "9c353c124d9841178a1c3837f080f533",
            "7065882e2b754d419f8a53b20c3e6b0f",
            "c309fcf97cc040169e6cea9e97bdfe41",
            "45cc1b52aa6943698eb96b7c3cb34195",
            "60b66500112149fabc13f2c787eac11d",
            "9f2cb52b8c914fe6b58d138324b36749"
          ]
        },
        "outputId": "0d8602d3-9400-483d-f600-d1de5d72bd2c"
      },
      "source": [
        "trainer = Trainer(\r\n",
        "    args=training_args,\r\n",
        "    train_dataset=train_dataset, \r\n",
        "    eval_dataset=test_dataset, \r\n",
        "    model_init=model_init,\r\n",
        "    compute_metrics=compute_metrics,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dddb62485b44a20977ad93afd114757",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2_t45WcGhOP"
      },
      "source": [
        "here you can define your search space.\r\n",
        "\r\n",
        "the my_hp_space function defines the hyper parameter super set, of which you can choose a subset (or even the whole set) for the grid search\r\n",
        "\r\n",
        "Note: You can include the opoch count as a hyperparameter, but this will drasticly increase the search space, I prefer setting a fixed epcoh size, then I manually search for the highest score between the epochs since optuna can't do that as far as I know. This should be easy as you won't be training for more than a handfull of epochs most likely, alternatively you can check out transformers.EarlyStoppingCallback in https://huggingface.co/transformers/main_classes/callback.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbDdRB86Enno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96e79d7-cc60-4051-bb20-2996087be15e"
      },
      "source": [
        "def my_hp_space(trial):\r\n",
        "    return {\r\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 7e-5, step=1e-5),\r\n",
        "        \"seed\": trial.suggest_categorical(\"seed\", [0, 1, 42, 666, 123, 12345]),\r\n",
        "        \"warmup_steps\": trial.suggest_int(\"warmup_steps\",0,total_steps*0.1,step=total_steps*0.1*0.5)\r\n",
        "    }\r\n",
        "\r\n",
        "search_space = {\r\n",
        "    \"learning_rate\":  list(np.arange(2e-5, 7e-5, 1e-5)),\r\n",
        "    \"seed\":  [0, 1, 42, 666, 123, 12345],\r\n",
        "    \"warmup_steps\": list(range(0, int((total_steps)*0.1)+1, int(total_steps*0.1*0.5)))\r\n",
        "}\r\n",
        "search_space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': [2e-05,\n",
              "  3.0000000000000004e-05,\n",
              "  4.000000000000001e-05,\n",
              "  5.000000000000001e-05,\n",
              "  6.000000000000001e-05],\n",
              " 'seed': [0, 1, 42, 666, 123, 12345],\n",
              " 'warmup_steps': [0, 22, 44]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSxmvKNaEpxU"
      },
      "source": [
        "def my_objective(metrics):\r\n",
        "    return metrics['eval_macro_f1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k3VJCgWGCoz"
      },
      "source": [
        "Make sure to mount your google drive here to save to drive. Otherwise change 'storage' to a local directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd9_6tiBEsMc"
      },
      "source": [
        "name = \"Test 1\"\r\n",
        "\r\n",
        "best_run = trainer.hyperparameter_search(direction=\"maximize\",\r\n",
        "                                         hp_space=my_hp_space,\r\n",
        "                                         compute_objective=my_objective,\r\n",
        "                                         n_trials=None,\r\n",
        "                                         pruner=optuna.pruners.NopPruner(),\r\n",
        "                                         sampler=optuna.samplers.GridSampler(search_space),\r\n",
        "                                         study_name=name,\r\n",
        "                                         storage=\"sqlite:////content/drive/MyDrive/{}.db\".format(name), #change this to a local directory if you want to save to disk\r\n",
        "                                         load_if_exists=False # you can change this to true, for continuing the search\r\n",
        "                                         )\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQH_lhfFpWHr"
      },
      "source": [
        "best_run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dob3csh85UhY"
      },
      "source": [
        "The hyperparameter_search supports both optuna and Raytune https://huggingface.co/blog/ray-tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfmnPQ4jGRBb"
      },
      "source": [
        "# Regular Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX2Re5kdHanh",
        "outputId": "ca205ce1-072a-4b10-d4ad-0d90db7b21ef"
      },
      "source": [
        "training_args = TrainingArguments(\"./train\")\r\n",
        "training_args.evaluate_during_training = True\r\n",
        "training_args.adam_epsilon = 1e-6\r\n",
        "training_args.learning_rate = 2e-5\r\n",
        "training_args.weight_decay = 0.01\r\n",
        "training_args.fp16 = True\r\n",
        "training_args.per_device_train_batch_size = 16\r\n",
        "training_args.per_device_eval_batch_size = 16\r\n",
        "training_args.gradient_accumulation_steps = 2\r\n",
        "training_args.num_train_epochs= 3\r\n",
        "\r\n",
        "\r\n",
        "steps_per_epoch = (len(selected_dataset.train)// (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps))\r\n",
        "total_steps = steps_per_epoch * training_args.num_train_epochs\r\n",
        "print(steps_per_epoch)\r\n",
        "print(total_steps)\r\n",
        "#Warmup_ratio\r\n",
        "warmup_ratio = 0.1\r\n",
        "training_args.warmup_steps = total_steps*warmup_ratio # or you can set the warmup steps directly \r\n",
        "\r\n",
        "training_args.evaluation_strategy = EvaluationStrategy.EPOCH\r\n",
        "#training_args.load_best_model_at_end = True\r\n",
        "#training_args.metric_for_best_model = 'accuracy'\r\n",
        "# training_args.logging_steps = 200\r\n",
        "training_args.save_steps = 100000 #don't want to save any model, there is probably a better way to do this :)\r\n",
        "training_args.seed = 25\r\n",
        "training_args.disable_tqdm = False\r\n",
        "training_args.lr_scheduler_type = 'cosine'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56\n",
            "168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr4uWLuXHixe",
        "outputId": "6cbdecc0-2a84-4f10-e603-93c251f2e27e"
      },
      "source": [
        "trainer = Trainer(\r\n",
        "    model = model_init(),\r\n",
        "    args = training_args,\r\n",
        "    train_dataset = train_dataset,\r\n",
        "    eval_dataset=test_dataset,\r\n",
        "    compute_metrics=compute_metrics,\r\n",
        "    #callbacks = [EarlyStoppingCallback(early_stopping_patience=3,early_stopping_threshold=0.001)]\r\n",
        ")\r\n",
        "#The warning message below is fine, it even tells you it's expected that this will happen since you are using a non-finetuned version of BERT and instantiating is\r\n",
        "#as as BertForSequenceClassification, which has some of its own special weights that are used for classification, this is normal since this is needed to finetune\r\n",
        "#on a downstream task, the message below even tells you so"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "l7xKAdYJH--T",
        "outputId": "841d4df3-bac0-42eb-b4f4-b4c6b7beb33a"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [171/171 01:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Macro F1</th>\n",
              "      <th>Macro F1 Pos Neg</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.436834</td>\n",
              "      <td>0.559729</td>\n",
              "      <td>0.364865</td>\n",
              "      <td>0.521062</td>\n",
              "      <td>0.615154</td>\n",
              "      <td>0.818985</td>\n",
              "      <td>2.788000</td>\n",
              "      <td>162.483000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.172677</td>\n",
              "      <td>0.926936</td>\n",
              "      <td>0.898512</td>\n",
              "      <td>0.926740</td>\n",
              "      <td>0.927605</td>\n",
              "      <td>0.953642</td>\n",
              "      <td>2.836300</td>\n",
              "      <td>159.713000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.138810</td>\n",
              "      <td>0.928077</td>\n",
              "      <td>0.900252</td>\n",
              "      <td>0.922676</td>\n",
              "      <td>0.935547</td>\n",
              "      <td>0.953642</td>\n",
              "      <td>2.867300</td>\n",
              "      <td>157.991000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning:\n",
            "\n",
            "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=171, training_loss=0.42305313913445725, metrics={'train_runtime': 102.2328, 'train_samples_per_second': 1.673, 'total_flos': 913658179355136, 'epoch': 3.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UWjJbjjIZJH"
      },
      "source": [
        "You can also add custom loss/training step etc by subclassing and overriding the default Trainer functions. See https://huggingface.co/transformers/main_classes/trainer.html. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsSN47U6A1H0"
      },
      "source": [
        "Once the model is trained we can save it along with it's corresponding tokenizer to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GEFLMRiuGwu",
        "outputId": "7861b60f-b5b8-4d8a-acd1-2b66dfbcea2c"
      },
      "source": [
        "trainer.save_model(\"TestModel\")\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "tokenizer.save_pretrained(\"TestModel\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('TestModel/tokenizer_config.json',\n",
              " 'TestModel/special_tokens_map.json',\n",
              " 'TestModel/vocab.txt',\n",
              " 'TestModel/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_I4DrlbA9rF"
      },
      "source": [
        "Now we can load the model back in and use huggingface's pipelines to get predictions, this saves us having to write our own prediction functions. Note that you have to save the model with its tokenizer otherwise the .from_pretrained function will error since it won't find some of the files it expects to see in the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae7zNisYuaRx"
      },
      "source": [
        "from transformers import pipeline, AutoModel\r\n",
        "model_name = \"TestModel\"\r\n",
        "num_labels=len(label_map)\r\n",
        "\r\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "\r\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, return_all_scores=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-pY7yAlx8Cn",
        "outputId": "da878999-8d84-4997-b563-3d322210c4aa"
      },
      "source": [
        "text = \"We saw a big increase in profits last year\"\r\n",
        "classifier(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'LABEL_0', 'score': 0.030508000403642654},\n",
              "  {'label': 'LABEL_1', 'score': 0.902646005153656},\n",
              "  {'label': 'LABEL_2', 'score': 0.06684602797031403}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUER7sODzNS0",
        "outputId": "1260be94-3d11-4071-bdac-0fa1eeac7355"
      },
      "source": [
        "model_name = \"bert-base-uncased\" #This is an example of what NOT to do, don't load a BERT model that isnt finetuned or a version that is\r\n",
        "# fine tuned on a different task to perform sentiment analysis, the warning message below should be a clear indicator that something is wrong.\r\n",
        "#its normal to see this message during training, not while using a pipeline to get predictions\r\n",
        "\r\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "\r\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, return_all_scores=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjn72S0xzXn2",
        "outputId": "ee1ff556-9655-4ef6-d0b9-64061053bb28"
      },
      "source": [
        "text = \"We saw a big increase in profits last year\"\r\n",
        "classifier(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'NEGATIVE', 'score': 0.0019726064056158066},\n",
              "  {'label': 'POSITIVE', 'score': 0.9980273842811584}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQFXiRIY07LF"
      },
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\r\n",
        "\r\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\r\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
        "\r\n",
        "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, return_all_scores=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyRTEZ1S08SM",
        "outputId": "9423abed-0df0-406c-8880-744d56c5c871"
      },
      "source": [
        "text = \"We saw a big increase in profits last year\"\r\n",
        "classifier(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': '1 star', 'score': 0.15750762820243835},\n",
              "  {'label': '2 stars', 'score': 0.18700267374515533},\n",
              "  {'label': '3 stars', 'score': 0.2065645456314087},\n",
              "  {'label': '4 stars', 'score': 0.2183638960123062},\n",
              "  {'label': '5 stars', 'score': 0.2305612415075302}]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeVU1qPg1Q8k"
      },
      "source": [
        "For a collection of models see https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQKcJ7B9K-wN"
      },
      "source": [
        "One proposed method to (possibly) improve training is ULMfit. To implement the ULMfit method, we need 3 things, Slanted Triangular Learning rates(SLTR), Discriminitive Finetuning and Gradual Unfreezing. I would highly recomend you look at Fastai for those implementations however they should be doable manually with HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-azfvZGLrdf"
      },
      "source": [
        "# SLTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgP3s9QiLuX4"
      },
      "source": [
        "# This is fairly easy to implement, and is actually a function of the optimizer, however since the training.args() \r\n",
        "# defaults to an instance of the AdamW optimizer with a linear scheduler all we have to change in our code is \r\n",
        "# change the following in the training.args\r\n",
        "\r\n",
        "training_args.lr_scheduler_type = 'cosine' #omit this line from above\r\n",
        "training_args.lr_scheduler_type = 'linear' # use this instead\r\n",
        "\r\n",
        "#It's not recomended to implement SLTR on it's own as it has been shown to lead to worse performance when alone, best used with the rest of ULMfit methods"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQqBvtRzMiWm"
      },
      "source": [
        "# Discriminitive Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQGRKQvLMwwy"
      },
      "source": [
        "# This is also a function of the optimizer, you can specify an optimizer in Trainer(), this accepts native pytorch/tensorflow optimizer, the following\r\n",
        "# is an example from the pytorch docs on how to implement this\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD([\r\n",
        "    {'params': model.base.parameters()},\r\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-3}\r\n",
        "], lr=1e-2, momentum=0.9)) \r\n",
        "\r\n",
        "#This means that model.base’s parameters will use the default learning rate set above, \r\n",
        "#model.classifier’s parameters will use a learning rate of 1e-3, and a momentum of 0.9 will be used for all parameters.\r\n",
        "#You should be able to extend this to further finetune the learning rate for each of the base.parameters\r\n",
        "#The same works for torch.optim.Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2kor6yrOeqy"
      },
      "source": [
        "# Gradual Unfreezing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBD2Tmm_OjSM"
      },
      "source": [
        "#This is possibly the trickiest to implement as training.train() runs the whole training and evaluation process in one go\r\n",
        "#The following does freeze the BERT layers with the exception of the classifier\r\n",
        "for param in model.bert.bert.parameters():\r\n",
        "    param.requires_grad = False\r\n",
        "#Note that this works with a model initialized as BertForSequenceClassification, if using AutoModel or BERTModel this would be slightly different so please\r\n",
        "#be wary of the specs of how your model is initialized\r\n",
        "\r\n",
        "#I am yet to test gradual unfreezing directly in huggingface but you should be able to do the above ^, call trainer.training_step() to perform one training step\r\n",
        "#and then unfreeze each of the BERT layers gradually from the top down, in our case we have 168 steps which we printed above in training_args. Also note that\r\n",
        "# .training_step() takes in two inputs, model and a dictionary of text and labels\r\n",
        "\r\n",
        "\r\n",
        "#As I said, look to fastai documentation to implement ULMfit, the docs are quite bad but fastai was developed by the authors of ULMfit it will likely perform better\r\n",
        "#than your manual implementation of the ULMfit method, it's somewhat tricky to get it working with huggingface pretrained models but there are some examples online \r\n",
        "#on Kaggle etc of how to do it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOSOGqJfQh0W"
      },
      "source": [
        "As I said, look to the fastai docs to implement ULMfit, they're not the best but fastai was developed by the authors of ULMfit so will likely perform better than your manual implementation of the ULMfit method, it's somewhat tricky to get it working with HuggingFace pretrained models but there are some examples online on Kaggle etc of how to do it.\r\n",
        "\r\n",
        "Finally, it is not a given that the ULMfit Method will lead to better performance since it was not specifically developed for BERT but rather for NLP task finetuning in general, Dogu Araci showed that it allowed for slightly better performance when training FinBERT(2-3% accuracy increase) however others have said that they did not experience a performance increase compared to a naive finetuning approach such as above. The following paper https://arxiv.org/pdf/2006.04884.pdf offers some insights on possibly why ULMfit might work for BERT and why it may not be the best method to go with and a very simple alternative finetuning approach.\r\n",
        "\r\n",
        "My recomendation is if you're trying to maximise your models performance - try everything, naive finetuning, ULMfit and the method proposed in the paper above then see which performs best, run a hyperparameter search for each method and run each of the hyperparameter searches 2-3 times to account for any randomness during training and see what gives you the best final result in terms of validation accuracy and loss. Ofcourse depending on the size of your training data and the compute power available to you this may not be realistically doable, you have to run each hyperparameter search only once, or ommit the hyperparameter search entirely(althought I would really not recomend that second approach).\r\n",
        "\r\n",
        "The differences will likely be small(in the range of 1-5%) which should not be surprising, given how good BERT already is at what it does(it performs close to the human benchmark, in some cases better) and the size of its original training corpus, it makes sense that it would be hard to optimize it further."
      ]
    }
  ]
}